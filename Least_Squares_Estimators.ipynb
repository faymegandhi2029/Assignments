{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Notation"
      ],
      "metadata": {
        "id": "-RMyEZHYUifs"
      },
      "id": "-RMyEZHYUifs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb597e19"
      },
      "source": [
        "### $Y$: the response variable\n",
        "\n",
        "### $y_i$: the observed value of the response variable for the $i$-th data point. In the context of regression, it's the actual measurement you have for a given predictor value $x_i$.\n",
        "\n",
        "<br>\n",
        "\n",
        "### $x_i$: This represents the value of the predictor variable for the $i$-th data point. These are the independent variables or features used to predict the response.\n",
        "\n",
        "<br>\n",
        "\n",
        "### $i$: an index that iterates through the individual data points or observations, typically from $1$ to $n$, where $n$ is the total number of observations.\n",
        "\n",
        "<br>\n",
        "\n",
        "### $f(x_i;\\boldsymbol{\\beta})$: model *function* that describes the expected value of the response variable at $x_i$, parameterized by the vector $\\boldsymbol{\\beta}$.\n",
        "\n",
        "For a linear model, this would be $\\beta_0 + \\beta_1 x_i$. It represents the systematic part of the relationship between $x$ and $y$.\n",
        "\n",
        "<br>\n",
        "\n",
        "### $\\boldsymbol{\\beta}$: a *vector* of the model parameters that we want to estimate from the data.\n",
        "\n",
        "For a simple linear regression, $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1)^\\top$, where $\\beta_0$ is the intercept and $\\beta_1$ is the slope.\n",
        "\n",
        "### $\\hat{\\boldsymbol{\\beta}}$: the *estimators* of $\\boldsymbol{\\beta}$\n",
        "\n",
        "<br>\n",
        "\n",
        "### $\\mathcal{L}(\\boldsymbol{\\beta})$: likelihood function.\n",
        "\n",
        "It quantifies how probable the observed data $(\\mathbf{y})$ are, given a specific set of parameter values ($\\boldsymbol{\\beta}$) and the assumed probability distribution of the noise (in this case, Gaussian). Maximizing the likelihood function with respect to $\\boldsymbol{\\beta}$ gives the maximum likelihood estimates of the parameters.\n",
        "\n",
        "<br>\n",
        "\n",
        "### $\\sigma_i$:  standard deviation of the noise or error for the $i$-th data point.\n",
        "\n",
        "* In the case of ordinary least squares (OLS), $\\sigma_i$ is assumed to be constant for all $i$ ($\\sigma_i = \\sigma$), representing *homoscedasticity*.\n",
        "\n",
        "* In weighted least squares (WLS), $\\sigma_i$ can vary for each data point, representing *heteroscedasticity*, and is assumed to be known or estimated independently. The weight $w_i$ is defined as $1/\\sigma_i^2$."
      ],
      "id": "cb597e19"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The model (assumptions)"
      ],
      "metadata": {
        "id": "AtPnCL1-Dab8"
      },
      "id": "AtPnCL1-Dab8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4wMnaMiUifs"
      },
      "source": [
        "Consider a series of independent observations $(x_i, y_i)$ for $i=1,\\dots,n$ for which the response variable is denoted by $Y$.\n",
        "* Model (linear in parameters):\n",
        "  \n",
        "  $$\n",
        "f(x_i;\\boldsymbol{\\beta}) \\;=\\; \\sum_{j=0}^{p-1} \\beta_j\\,\\phi_j(x_i),\n",
        "$$\n",
        "  \n",
        "  where $\\{\\phi_j\\}$ are chosen basis functions (e.g., $\\phi_0=1$, $\\phi_1(x)=x$ for a line), and $\\boldsymbol{\\beta}=(\\beta_0, \\dots, \\beta_{p-1})^\\top$ is the parameter vector. The model is thus of the form $Y = \\beta_0 + \\beta_1 x + \\epsilon$, where $\\epsilon$ is the random error. An observation would thus be $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$.\n",
        "  \n",
        "* Noise model (possibly heteroscedastic):\n",
        "  \n",
        "  $$\n",
        "Y_i \\mid x_i \\;\\sim\\; \\mathcal{N}\\!\\big(f(x_i;\\boldsymbol{\\beta}),\\,\\sigma_i^2\\big).\n",
        "$$\n",
        "* Likelihood $\\mathcal{L}$, log-likelihood $\\mathcal{W}=\\log \\mathcal{L}$, least-squares sum $\\mathcal{M}$.\n",
        "* Estimators are indicated with a caret (e.g., $\\hat{\\boldsymbol{\\beta}}$). Averages use bars (e.g., $\\bar{x}$). The residual for each observation pair $(x_i, y_i)$ is $e_i = y_i - \\hat{y}_i = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)$.\n",
        "\n"
      ],
      "id": "e4wMnaMiUifs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weighted Least Squares derived from Gaussian likelihood"
      ],
      "metadata": {
        "id": "3baknBO1DkYG"
      },
      "id": "3baknBO1DkYG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGLsybsOUifs"
      },
      "source": [
        "The likelihood function for independent normally distributed measurements is given by:\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\boldsymbol{\\beta}) \\;=\\; \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}\\,\\sigma_i}\n",
        "\\exp\\!\\left[-\\frac{\\big(y_i - f(x_i;\\boldsymbol{\\beta})\\big)^2}{2\\sigma_i^2}\\right].\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Note that $\\boldsymbol{\\beta}$ is a vector with the parameters we want to estimate. The log likelihood is then:\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        "\\mathcal{W}(\\boldsymbol{\\beta}) \\;=\\; \\sum_{i=1}^n \\left[-\\tfrac{1}{2}\\log(2\\pi)-\\log\\sigma_i\n",
        "-\\frac{\\big(y_i - f(x_i;\\boldsymbol{\\beta})\\big)^2}{2\\sigma_i^2}\\right].\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Maximizing $\\mathcal{W}$ is equivalent to minimizing\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        "\\mathcal{M}(\\boldsymbol{\\beta})\n",
        "\\;=\\; \\sum_{i=1}^n w_i\\,\\big(y_i - f(x_i;\\boldsymbol{\\beta})\\big)^2,\n",
        "\\qquad\n",
        "w_i \\;=\\; \\frac{1}{\\sigma_i^{2}}.\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "This is **weighted least squares (WLS)**. If all $\\sigma_i=\\sigma$, the common factor cancels and we recover **ordinary least squares (OLS)**.\n",
        "\n"
      ],
      "id": "eGLsybsOUifs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alternative notation to match textbook: Least Squares with $\\bar y_i$(pointwise mean) notation"
      ],
      "metadata": {
        "id": "aeBRdx7ytKJy"
      },
      "id": "aeBRdx7ytKJy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl8Dr9KqUifs"
      },
      "source": [
        "### Setup and notation\n",
        "\n",
        "* Observations: $(x_i, y_i)$ for $i=1,\\dots,n$, independent. The response variable is denoted by $Y$.\n",
        "* True/noiseless value at $x_i$:\n",
        "$$\n",
        "\\bar y_i \\;=\\; f(x_i;\\boldsymbol{\\beta}) \\;=\\; \\sum_{j=0}^{p-1} \\beta_j\\,\\phi_j(x_i).\n",
        "$$\n",
        "* Noise model (possibly heteroscedastic):\n",
        "$$\n",
        "Y_i \\mid x_i \\;\\sim\\; \\mathcal{N}\\!\\big(\\bar y_i,\\,\\sigma_i^2\\big).\n",
        "$$\n",
        "* Likelihood $\\mathcal L$, log-likelihood $\\mathcal W=\\log\\mathcal L$, least-squares sum $\\mathcal M$.\n",
        "* Estimators carry a caret (e.g., $\\hat{\\boldsymbol{\\beta}}$). **Warning on bars:** $\\bar y_i$ is the *mean at point $i$*; $\\bar y$ is the *sample average* of the $y_i$â€™s. The residual for each observation pair $(x_i, y_i)$ is $e_i = y_i - \\hat{y}_i = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)$.\n",
        "\n",
        "### From Gaussian likelihood to (weighted) least squares\n",
        "\n",
        "Independence gives\n",
        "$$\n",
        "\\mathcal L(\\boldsymbol{\\beta})\n",
        "=\\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}\\,\\sigma_i}\n",
        "\\exp\\!\\left[-\\frac{(y_i-\\bar y_i)^2}{2\\sigma_i^2}\\right].\n",
        "$$\n",
        "\n",
        "Log:\n",
        "$$\n",
        "\\mathcal W(\\boldsymbol{\\beta})\n",
        "=\\sum_{i=1}^n\\left[-\\tfrac{1}{2}\\log(2\\pi)-\\log\\sigma_i\n",
        "-\\frac{(y_i-\\bar y_i)^2}{2\\sigma_i^2}\\right].\n",
        "$$\n",
        "\n",
        "Maximizing $\\mathcal W$ is equivalent to minimizing\n",
        "$$\n",
        "\\boxed{\n",
        "\\mathcal M(\\boldsymbol{\\beta})\n",
        "=\\sum_{i=1}^n w_i\\,(y_i-\\bar y_i)^2,\n",
        "\\qquad w_i=\\sigma_i^{-2}\n",
        "}.\n",
        "$$"
      ],
      "id": "pl8Dr9KqUifs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Assignment"
      ],
      "metadata": {
        "id": "j6MZzIWn6YM5"
      },
      "id": "j6MZzIWn6YM5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YSJzyGk6Woy"
      },
      "source": [
        "## Assignment as written\n",
        "\n",
        "Create an Excel (or Google) spreadsheet to fit a set of $n=8-12$ data points to a line. For $x_i$, use the index 1 through $n$. Generate your model $y_i$ data with a slope $\\beta_1$ between 1 and 3 (you choose) and an intercept $\\beta_0$ between -1 and 1 (you choose).\n",
        "\n",
        "Create two worksheets. The first worksheet should estimate $\\beta_0$ and $\\beta_1$ assuming a constant variance of 1, i.e. ordinary least squares. The second worksheet should carry out a weighted least squares fit assuming that the variance is equal to $y_i+\\epsilon$. Compare the values for the slope and intercept in each case, and comment on your observations.\n",
        "\n",
        "## Gemini interpretation of the assignment\n",
        "\n",
        "The goal of this assignment is to explore Ordinary Least Squares (OLS) and Weighted Least Squares (WLS) regression in the presence of heteroscedastic noise. We will generate synthetic data with a known underlying linear relationship and non-constant noise variance. Then, we will perform both OLS and WLS on this data, using the known variances for the weights in WLS. Finally, we will compare the results of the two methods, paying close attention to the estimated parameters and their variances."
      ],
      "id": "5YSJzyGk6Woy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehRPuZkqUift"
      },
      "source": [
        "# OLS for a straight line with constant variance\n",
        "\n",
        "Take $\\phi_0(x)=1$, $\\phi_1(x)=x$; parameters $\\beta_0$ (intercept), $\\beta_1$ (slope). With constant $\\sigma$, $W\\propto I$ and the proportionality cancels. Define\n",
        "\n",
        "$$\n",
        "\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i,\\qquad\n",
        "\\bar{y}=\\frac{1}{n}\\sum_{i=1}^n y_i,\n",
        "$$\n",
        "\n",
        "$$\n",
        "S_{xx}=\\sum_{i=1}^n (x_i-\\bar{x})^2,\\qquad\n",
        "S_{xy}=\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y}).\n",
        "$$\n",
        "\n",
        "### OLS estimators\n",
        "\n",
        "$$\n",
        "\\boxed{\\,\\hat{\\beta}_1=\\frac{S_{xy}}{S_{xx}},\\qquad \\hat{\\beta}_0=\\bar{y}-\\hat{\\beta}_1\\,\\bar{x}\\,}.\n",
        "$$\n",
        "\n",
        "### OLS Estimators with explicit sums:\n",
        "\n",
        "$$\n",
        "\\boxed{\\,\\hat{\\beta}_1=\\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2},\\qquad \\hat{\\beta}_0=\\bar{y}-\\hat{\\beta}_1\\,\\bar{x}\\,}.\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "### Variance Estimates\n",
        "\n",
        "- The fitted pointwise means are $\\hat{y}_i=\\bar y_i(\\hat{\\boldsymbol{\\beta}})=\\hat{\\beta}_0+\\hat{\\beta}_1 x_i$\n",
        "\n",
        "- Residuals are $e_i=y_i-\\hat{y}_i =y_i-(\\hat{\\beta}_0+\\hat{\\beta}_1 x_i)$.\n",
        "\n",
        "- $\\mathrm{SSE}=\\sum e_i^2$.\n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        "\\widehat{\\sigma}^2_{\\mathrm{MLE}}=\\frac{\\mathrm{SSE}}{n},\n",
        "\\qquad\n",
        "\\widehat{\\sigma}^2=\\frac{\\mathrm{SSE}}{n-2}\\quad(\\text{unbiased; here }p=2).\n",
        "$$\n",
        "\n"
      ],
      "id": "ehRPuZkqUift"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OLS Estimator Expressions without the averages:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y}) = \\sum_{i=1}^n (x_i y_i - x_i \\bar{y} - \\bar{x} y_i + \\bar{x}\\bar{y}) = \\sum x_i y_i - \\bar{y}\\sum x_i - \\bar{x}\\sum y_i + \\sum \\bar{x}\\bar{y} = \\sum x_i y_i - \\frac{\\sum y_i}{n}\\sum x_i - \\frac{\\sum x_i}{n}\\sum y_i + n \\frac{\\sum x_i}{n}\\frac{\\sum y_i}{n} = \\sum x_i y_i - \\frac{(\\sum x_i)(\\sum y_i)}{n} - \\frac{(\\sum x_i)(\\sum y_i)}{n} + \\frac{(\\sum x_i)(\\sum y_i)}{n} = \\sum x_i y_i - \\frac{(\\sum x_i)(\\sum y_i)}{n}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^n (x_i-\\bar{x})^2 = \\sum_{i=1}^n (x_i^2 - 2x_i\\bar{x} + \\bar{x}^2) = \\sum x_i^2 - 2\\bar{x}\\sum x_i + \\sum \\bar{x}^2 = \\sum x_i^2 - 2\\frac{\\sum x_i}{n}\\sum x_i + n\\left(\\frac{\\sum x_i}{n}\\right)^2 = \\sum x_i^2 - \\frac{2(\\sum x_i)^2}{n} + \\frac{(\\sum x_i)^2}{n} = \\sum x_i^2 - \\frac{(\\sum x_i)^2}{n}\n",
        "$$\n",
        "\n",
        "So,\n",
        "\n",
        "$$\n",
        "\\boxed{\\,\\hat{\\beta}_1=\\frac{\\sum_{i=1}^n x_i y_i - \\frac{1}{n}(\\sum x_i)(\\sum y_i)}{\\sum_{i=1}^n x_i^2 - \\frac{1}{n}(\\sum x_i)^2},\\qquad \\hat{\\beta}_0=\\bar{y}-\\hat{\\beta}_1\\,\\bar{x}\\,}.\n",
        "$$\n"
      ],
      "metadata": {
        "id": "pzkhfk1Kyrgp"
      },
      "id": "pzkhfk1Kyrgp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Weighted straight line (known $\\sigma_i$)\n",
        "\n",
        "## Expressions with average values\n",
        "\n",
        "With $w_i=1/\\sigma_i^2$,\n",
        "\n",
        "$$\n",
        "\\bar{x}_w=\\frac{\\sum_i w_i x_i}{\\sum_i w_i},\\qquad\n",
        "\\bar{y}_w=\\frac{\\sum_i w_i y_i}{\\sum_i w_i},\n",
        "$$\n",
        "\n",
        "$$\n",
        "S_{xx,w}=\\sum_i w_i (x_i-\\bar{x}_w)^2,\\qquad\n",
        "S_{xy,w}=\\sum_i w_i (x_i-\\bar{x}_w)(y_i-\\bar{y}_w),\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\boxed{\\,\\hat{\\beta}_1=\\frac{S_{xy,w}}{S_{xx,w}},\\qquad \\hat{\\beta}_0=\\bar{y}_w-\\hat{\\beta}_1\\,\\bar{x}_w\\,}.\n",
        "$$"
      ],
      "metadata": {
        "id": "f9ncUGDK0OKx"
      },
      "id": "f9ncUGDK0OKx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expressions without averages\n",
        "\n",
        "$$\n",
        "S_{xy,w} = \\sum_i w_i (x_i-\\bar{x}_w)(y_i-\\bar{y}_w) = \\sum_i w_i (x_i y_i - x_i \\bar{y}_w - \\bar{x}_w y_i + \\bar{x}_w \\bar{y}_w) = \\sum_i w_i x_i y_i - \\bar{y}_w \\sum_i w_i x_i - \\bar{x}_w \\sum_i w_i y_i + \\bar{x}_w \\bar{y}_w \\sum_i w_i\n",
        "$$\n",
        "Substitute $\\bar{x}_w = \\frac{\\sum_i w_i x_i}{\\sum_i w_i}$ and $\\bar{y}_w = \\frac{\\sum_i w_i y_i}{\\sum_i w_i}$:\n",
        "$$\n",
        "S_{xy,w} = \\sum_i w_i x_i y_i - \\left(\\frac{\\sum_i w_i y_i}{\\sum_i w_i}\\right) \\sum_i w_i x_i - \\left(\\frac{\\sum_i w_i x_i}{\\sum_i w_i}\\right) \\sum_i w_i y_i + \\left(\\frac{\\sum_i w_i x_i}{\\sum_i w_i}\\right) \\left(\\frac{\\sum_i w_i y_i}{\\sum_i w_i}\\right) \\sum_i w_i\n",
        "$$\n",
        "$$\n",
        "S_{xy,w} = \\sum_i w_i x_i y_i - \\frac{(\\sum_i w_i y_i)(\\sum_i w_i x_i)}{\\sum_i w_i} - \\frac{(\\sum_i w_i x_i)(\\sum_i w_i y_i)}{\\sum_i w_i} + \\frac{(\\sum_i w_i x_i)(\\sum_i w_i y_i)}{\\sum_i w_i} = \\sum_i w_i x_i y_i - \\frac{(\\sum_i w_i x_i)(\\sum_i w_i y_i)}{\\sum_i w_i}\n",
        "$$\n",
        "\n",
        "$$\n",
        "S_{xx,w} = \\sum_i w_i (x_i-\\bar{x}_w)^2 = \\sum_i w_i (x_i^2 - 2x_i\\bar{x}_w + \\bar{x}_w^2) = \\sum_i w_i x_i^2 - 2\\bar{x}_w \\sum_i w_i x_i + \\bar{x}_w^2 \\sum_i w_i\n",
        "$$\n",
        "\n",
        "Substitute $\\bar{x}_w = \\frac{\\sum_i w_i x_i}{\\sum_i w_i}$:\n",
        "\n",
        "$$\n",
        "S_{xx,w} = \\sum_i w_i x_i^2 - 2\\left(\\frac{\\sum_i w_i x_i}{\\sum_i w_i}\\right) \\sum_i w_i x_i + \\left(\\frac{\\sum_i w_i x_i}{\\sum_i w_i}\\right)^2 \\sum_i w_i\n",
        "$$\n",
        "$$\n",
        "S_{xx,w} = \\sum_i w_i x_i^2 - \\frac{2(\\sum_i w_i x_i)^2}{\\sum_i w_i} + \\frac{(\\sum_i w_i x_i)^2}{\\sum_i w_i} = \\sum_i w_i x_i^2 - \\frac{(\\sum_i w_i x_i)^2}{\\sum_i w_i}\n",
        "$$\n",
        "\n",
        "So, the weighted least squares estimators are:\n",
        "\n",
        "$$\n",
        "\\boxed{\\,\\hat{\\beta}_1=\\frac{\\sum_i w_i x_i y_i - \\frac{(\\sum_i w_i x_i)(\\sum_i w_i y_i)}{\\sum_i w_i}}{\\sum_i w_i x_i^2 - \\frac{(\\sum_i w_i x_i)^2}{\\sum_i w_i}},\\qquad \\hat{\\beta}_0=\\bar{y}_w-\\hat{\\beta}_1\\,\\bar{x}_w\\,}.\n",
        "$$"
      ],
      "metadata": {
        "id": "mc_mKDqw0atQ"
      },
      "id": "mc_mKDqw0atQ"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-RMyEZHYUifs",
        "AtPnCL1-Dab8",
        "3baknBO1DkYG",
        "aeBRdx7ytKJy",
        "ehRPuZkqUift"
      ]
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}